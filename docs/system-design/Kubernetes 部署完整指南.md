# è™šæ‹Ÿæœº



é‡å¯ä¼šæ”¹å˜ip DHCP NAT VMware-8





[VMwareè™šæ‹Ÿæœºéƒ¨ç½²k8sé›†ç¾¤_vmqk18-CSDNåšå®¢](https://blog.csdn.net/qq_41860461/article/details/122418639)



[K8Sæ„å»º1å°master2å°node+Harbor - ä¸€ä»£è‚å¸ - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/yyq1/p/13991453.html)





[Ubuntu18.04ä¸‹å®‰è£…é…ç½®SSHæœåŠ¡_ubuntu18.04 ssh yrs-CSDNåšå®¢](https://blog.csdn.net/wgc0802402/article/details/91046196)





[VMware è™šæ‹Ÿæœºç½‘ç»œé…ç½® ã€100%è§£å†³ã€‘ã€è¶…è¯¦ç»†ã€‘_vmwareè™šæ‹Ÿæœºç½‘ç»œé…ç½®-CSDNåšå®¢](https://blog.csdn.net/weixin_56261190/article/details/144807447)

[VMwareè™šæ‹Ÿæœºå’Œä¸»æœºé—´å¤åˆ¶ç²˜è´´å…±äº«å‰ªè´´æ¿ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/665154528#:~:text=å®‰è£…open-vm-toolså®‰è£…open-vm-tools-desktopå¦‚å›¾å¼€å¯è™šæ‹Ÿæœºè®¾ç½®)



[(14 æ¡æ¶ˆæ¯) ã€Ubuntuã€‘Ubuntu 18.04 LTS æ›´æ¢å›½å†…æºâ€”â€”è§£å†³ç»ˆç«¯ä¸‹è½½é€Ÿåº¦æ…¢çš„é—®é¢˜ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/61228593#:~:text=æœ€è¿‘è£…äº†ubuntu18.04 LTSï¼Œä¸‹è½½è½¯ä»¶æœ‰ç‚¹æ…¢ï¼Œç½‘ä¸Šæœäº†ä¸‹è§£å†³æ–¹æ¡ˆï¼Œå¤§è‡´æ˜¯ä¸¤ç§ï¼šä¸€ã€æŠŠ/etc/apt/sources.listæ–‡ä»¶é‡Œçš„æºæ›´æ¢ä¸€ä¸‹ï¼Œæ”¹æˆé˜¿é‡Œäº‘æˆ–è€…å…¶å®ƒçš„é•œåƒçš„æ–‡ä»¶ï¼›äºŒã€æ›´æ¢software&updatesé‡Œçš„select)





```
network:
  ethernets:
    ens33:  # ç½‘å¡åç§°ï¼Œä¸ ip addr æ˜¾ç¤ºçš„ä¸€è‡´
      dhcp4: no  # å…³é—­ DHCP
      addresses: [192.168.66.10/24]  # é™æ€ IP åŠå­ç½‘æ©ç ï¼ˆ/24 å¯¹åº” 255.255.255.0ï¼‰
      gateway4: 192.168.66.1  # ç½‘å…³ï¼ˆä»ä¹‹å‰çš„è·¯ç”±ä¿¡æ¯æ¨æµ‹ï¼Œé€šå¸¸æ˜¯ NAT æ¨¡å¼çš„è™šæ‹Ÿç½‘å…³ï¼‰
      nameservers:
        addresses: [8.8.8.8, 223.6.6.6]  # DNS æœåŠ¡å™¨
  version: 2
  renderer: networkd
```



```bash
sudo vim /etc/netplan/01-network-manager-all.yaml
```

### å››ã€æ­£ç¡®é…ç½®ï¼šè®©é™æ€ IP ç”Ÿæ•ˆ

è‹¥è¦ä½¿ç”¨ `netplan` é…ç½®é™æ€ IPï¼ˆè€Œéå›¾å½¢ç•Œé¢ï¼‰ï¼Œéœ€è°ƒæ•´ `01-network-manager-all.yaml`ï¼Œè®©ç‰¹å®šæ¥å£ç”± `networkd` ç®¡ç†ï¼Œé¿å…å†²çªã€‚

#### æ­¥éª¤ 1ï¼šç¼–è¾‘ `01-network-manager-all.yaml`

bas

```bash
sudo vim /etc/netplan/01-network-manager-all.yaml
```

#### æ­¥éª¤ 2ï¼šä¿®æ”¹é…ç½®ï¼ˆå…³é”®ï¼‰

ä¿ç•™ `NetworkManager` ç®¡ç†å…¶ä»–æ¥å£ï¼ˆå¦‚éœ€ï¼‰ï¼Œä½†æŒ‡å®š `ens33` ç”± `networkd` ç®¡ç†ï¼ˆç”¨äºé™æ€ IPï¼‰ï¼š

yaml











```yaml
network:
  version: 2
  renderer: NetworkManager  # å…¶ä»–æ¥å£ä»ç”± NetworkManager ç®¡ç†
  ethernets:
    ens33:  # é’ˆå¯¹ä½ çš„ç½‘å¡ï¼ˆens33ï¼‰å•ç‹¬æŒ‡å®šæ¸²æŸ“å™¨
      renderer: networkd  # ç”¨ networkd ç®¡ç†è¯¥æ¥å£ï¼ˆé€‚åˆé™æ€ IPï¼‰
```

#### æ­¥éª¤ 3ï¼šç¡®ä¿ `01-netcfg.yaml` é…ç½®æ­£ç¡®

ç¼–è¾‘ `01-netcfg.yaml`ï¼ˆè‹¥ä¸å­˜åœ¨å¯åˆ›å»ºï¼‰ï¼Œé…ç½®é™æ€ IPï¼š

yaml











```yaml
network:
  version: 2
  renderer: networkd  # ä¸ä¸Šä¸€æ­¥çš„ ens33 æ¸²æŸ“å™¨ä¸€è‡´
  ethernets:
    ens33:
      dhcp4: no  # å…³é—­ DHCP
      addresses: [192.168.66.130/24]  # é™æ€ IP
      gateway4: 192.168.66.1  # ç½‘å…³ï¼ˆä» ip route ç¡®è®¤ï¼‰
      nameservers:
        addresses: [223.5.5.5, 223.6.6.6]  # DNS
```

#### æ­¥éª¤ 4ï¼šåº”ç”¨é…ç½®å¹¶éªŒè¯

bash











```bash
# æ£€æŸ¥é…ç½®æ ¼å¼é”™è¯¯
sudo netplan try

# åº”ç”¨é…ç½®
sudo netplan apply

# é‡å¯ç½‘ç»œæœåŠ¡
sudo systemctl restart systemd-networkd

# éªŒè¯ IP æ˜¯å¦ç”Ÿæ•ˆ
ip addr show ens33
```



echo "192.168.66.10 master
192.168.66.11 node01
192.168.66.12 node02"/>> /etc/hosts



```
# master èŠ‚ç‚¹
sudo hostnamectl set-hostname k8s-master01

# node1 èŠ‚ç‚¹
sudo hostnamectl set-hostname k8s-node01

# node2 èŠ‚ç‚¹
sudo hostnamectl set-hostname k8s-node02

# Harbor èŠ‚ç‚¹
sudo hostnamectl set-hostname hub

# éªŒè¯ä¸»æœºå
hostname
```





depends:        nf_defrag_ipv6,libcrc32c,nf_defrag_ipv4





### 3. **èƒ½å¦é€šè¿‡é™çº§ K8s ç‰ˆæœ¬æ¥ä½¿ç”¨ Docker-CEï¼Ÿ**

âœ… **å¯ä»¥ï¼Œä½†ä»…é™äº Kubernetes â‰¤ 1.23 ç‰ˆæœ¬ã€‚**

| Kubernetes ç‰ˆæœ¬ | æ˜¯å¦æ”¯æŒ Docker-CE        | è¯´æ˜                        |
| --------------- | ------------------------- | --------------------------- |
| â‰¤ 1.23          | âœ… æ”¯æŒï¼ˆé€šè¿‡ dockershimï¼‰ | å¯ä»¥ä½¿ç”¨ `docker://` è¿è¡Œæ—¶ |
| â‰¥ 1.24          | âŒ ä¸æ”¯æŒ                  | `dockershim` å·²ç§»é™¤         |





#### ä½¿ç”¨å…¶ä»–å®¹å™¨è¿›è¡Œéƒ¨ç½²

containerd ä½¿ç”¨æ–‡æ¡£

é…ç½®ç¬¬ä¸‰æ–¹é•œåƒ

[Containerdé…ç½®å›½å†…é•œåƒæº - CharyGao - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/Chary/articles/18701060)

[containerdå®¹å™¨è¿è¡Œæ—¶å¿«é€Ÿå…¥é—¨ä½¿ç”¨æŒ‡å— - å°¹æ­£æ° - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/yinzhengjie/p/18058010)



ä¿®æ”¹åŸå…ˆéƒ¨ç½²æ–‡æ¡£



ä½¿ç”¨æ–°ç‰ˆæœ¬k8sè¿›è¡Œéƒ¨ç½² container



```
WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
ERRO[0000] validate service connection: validate CRI v1 runtime API for endpoint "unix:///var/run/dockershim.sock": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory" 
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.12-0ubuntu1~18.04.1
RuntimeApiVersion:  v1
æ£€æŸ¥é”™è¯¯

ä»é”™è¯¯ä¿¡æ¯æ¥çœ‹ï¼Œcrictl æ— æ³•æ­£å¸¸è¿æ¥å®¹å™¨è¿è¡Œæ—¶ï¼ˆdockershim ä¸å¯ç”¨ï¼Œä½†æ£€æµ‹åˆ°äº† containerdï¼‰ï¼Œä¸»è¦åŸå› æ˜¯ K8s 1.24+ å·²ç§»é™¤ dockershimï¼Œè€Œä½ çš„ç¯å¢ƒä¸­å®é™…ä½¿ç”¨çš„æ˜¯ containerd ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ï¼Œä½† crictl é»˜è®¤é…ç½®ä»æŒ‡å‘ dockershimï¼Œå¯¼è‡´è¿æ¥å¤±è´¥ã€‚ä»¥ä¸‹æ˜¯å…·ä½“è§£å†³æ­¥éª¤ï¼š
```



```
æ­¥éª¤ 3ï¼šéªŒè¯é…ç½®æ˜¯å¦ç”Ÿæ•ˆ
å†æ¬¡æ‰§è¡Œ crictl versionï¼Œè‹¥è¾“å‡ºç±»ä¼¼ä»¥ä¸‹å†…å®¹ï¼Œè¯´æ˜è¿æ¥æˆåŠŸï¼š
plaintext
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.12-0ubuntu1~18.04.1
RuntimeApiVersion:  v1
æ­¥éª¤ 4ï¼šæµ‹è¯• crictl åŠŸèƒ½
bash
# æŸ¥çœ‹é•œåƒåˆ—è¡¨ï¼ˆä¸ `ctr images list` ç±»ä¼¼ï¼‰
crictl images

# æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„å®¹å™¨
crictl ps
```



scp sz rz

[Linuxæ–‡ä»¶ä¼ è¾“è¯¦è§£:rzã€szä¸scpå‘½ä»¤_rzå‘½ä»¤ â€“ èœé¸Ÿ-åˆ›ä½œä½ çš„åˆ›ä½œ (52runoob.com)](https://www.52runoob.com/archives/4832)





ä¾‹å¦‚ï¼šæœ¬åœ°å½“å‰ç”¨ `user1` ç™»å½•ï¼Œæ‰§è¡Œä¸Šè¿°å‘½ä»¤æ—¶ï¼Œä¼šé»˜è®¤å°è¯•ç”¨ `user1@192.168.1.100` ç™»å½•è¿œç¨‹æœåŠ¡å™¨ã€‚

âš ï¸ æ³¨æ„ï¼šè‹¥è¿œç¨‹æœåŠ¡å™¨ä¸å­˜åœ¨ä¸æœ¬åœ°ç›¸åŒçš„ç”¨æˆ·åï¼ˆå¦‚æœ¬åœ° `user1`ï¼Œè¿œç¨‹æ—  `user1`ï¼‰ï¼Œä¼šç›´æ¥æŠ¥é”™ `Permission denied` æˆ– `No such user`ã€‚





è¿™æ˜¯ `scp` é¦–æ¬¡è¿æ¥è¿œç¨‹ä¸»æœº `192.168.66.11` æ—¶çš„æ­£å¸¸å®‰å…¨éªŒè¯æç¤ºï¼Œç›®çš„æ˜¯ç¡®è®¤è¿œç¨‹ä¸»æœºèº«ä»½ï¼Œé˜²æ­¢ â€œä¸­é—´äººæ”»å‡»â€ã€‚åç»­è¾“å‡ºåŠæ“ä½œæ­¥éª¤å¦‚ä¸‹ï¼š





kubectlå¯åŠ¨å¤±è´¥

```
10æœˆ 01 20:09:22 k8s-master01 kubelet[15723]: E1001 20:09:22.444493   15723 run.go:74] "command failed" err="failed to load kubelet config file, path: /var/lib/kubelet/config.yaml, error: failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file \"/var/lib/kubelet/config.yaml\", error: open /var/lib/kubelet/config.yaml: no such file or directory"
10æœˆ 01 20:09:22 k8s-master01 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
10æœˆ 01 20:09:22 k8s-master01 systemd[1]: kubelet.service: Failed with result 'exit-code'.
10æœˆ 01 20:09:32 k8s-master01 systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
10æœˆ 01 20:09:32 k8s-master01 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 37.
10æœˆ 01 20:09:32 k8s-master01 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
10æœˆ 01 20:09:32 k8s-master01 systemd[1]: Started kubelet: The Kubernetes Node Agent.
10æœˆ 01 20:09:32 k8s-master01 kubelet[15754]: E1001 20:09:32.688448   15754 run.go:74] "command failed" err="failed to load kubelet config file, path: /var/lib/kubelet/config.yaml, error: failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file \"/var/lib/kubelet/config.yaml\", error: open /var/lib/kubelet/config.yaml: no such file or directory"
10æœˆ 01 20:09:32 k8s-master01 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
10æœˆ 01 20:09:32 k8s-master01 systemd[1]: kubelet.service: Failed with result 'exit-code'.

```



```
# æŸ¥çœ‹å¯ç”¨ç‰ˆæœ¬ï¼ˆç¡®è®¤ 1.28.2 å¯ç”¨ï¼‰
apt-cache madison kubeadm | grep -E '1\.28\.2|1\.28\.1'

# å®‰è£… 1.28.2 ç‰ˆæœ¬ï¼ˆå…³é”®ä¿®æ­£ï¼‰
VERSION=1.28.2-00
sudo apt install -y kubeadm=$VERSION kubelet=$VERSION kubectl=$VERSION

# é”å®šç‰ˆæœ¬ï¼ˆé˜²æ­¢è‡ªåŠ¨å‡çº§ï¼‰
sudo apt-mark hold kubeadm kubelet kubectl

# å¯ç”¨å¹¶å¯åŠ¨ kubelet
sudo systemctl enable --now kubelet
```

#### 2. éªŒè¯å®‰è£…ï¼ˆå…³é”®éªŒè¯ï¼‰





ä½¿ç”¨ä¸€é”®å¼è¿›è¡Œéƒ¨ç½²

```
# 1. æ£€æŸ¥ K8s ç‰ˆæœ¬
kubectl version --client --short
# è¾“å‡ºï¼šClient Version: v1.28.2

# 2. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes
# è¾“å‡ºï¼šk8s-master01   Ready   ...   v1.28.2

# 3. æ£€æŸ¥ Flannel çŠ¶æ€
kubectl get pods -n kube-system -l k8s-app=flannel
# è¾“å‡ºï¼škube-flannel-ds-...   Running
```



#### é…ç½®åŠ è½½æ¨¡å—

```
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```

> è¿™ä¸ªé”™è¯¯æ˜¯å› ä¸º Linux å†…æ ¸æ¨¡å— br_netfilter æ²¡æœ‰åŠ è½½ï¼Œå¯¼è‡´ /proc/sys/net/bridge/bridge-nf-call-iptables ä¸å­˜åœ¨ã€‚
>
> è¿™æ˜¯ Kubernetes çš„å¸¸è§å‰ç½®æ£€æŸ¥é¡¹ï¼Œå¿…é¡»ä¿®å¤ï¼ˆä¸èƒ½ç®€å•å¿½ç•¥ï¼‰ï¼Œå¦åˆ™ Pod ç½‘ç»œä¼šå¼‚å¸¸ã€‚



### Haboré•œåƒä»“åº“è®¾ç½®

```

å…«ã€éƒ¨ç½² Harbor é•œåƒä»“åº“ï¼ˆv2.11ï¼‰
ï¼ˆHarbor é…ç½®ä¿æŒä¸å˜ï¼Œä½†éœ€ç¡®ä¿ Docker ä¾èµ–å·²ç§»é™¤ï¼‰

1. å®‰è£… Docker Compose
Bash
ç¼–è¾‘
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose --version
2. è§£å‹ Harbor å®‰è£…åŒ…ï¼ˆv2.11.0ï¼‰
Bash
ç¼–è¾‘
cd /root
tar xzvf harbor-offline-installer-v2.11.0.tgz
sudo mv harbor /usr/local/
3. é…ç½® Harborï¼ˆå…³é”®ï¼šå¯ç”¨ insecure-registriesï¼‰
Bash
ç¼–è¾‘
cd /usr/local/harbor
sudo vim harbor.cfg
Ini
ç¼–è¾‘
hostname = hub.yyq.com
ui_url_protocol = https
db_password = root123
ssl_cert = /data/cert/server.crt
ssl_cert_key = /data/cert/server.key
harbor_admin_password = Harbor12345
# æ·»åŠ ä»¥ä¸‹é…ç½®ï¼ˆè®© K8s èŠ‚ç‚¹ä¿¡ä»» Harborï¼‰
insecure_registry = hub.yyq.com
4. ç”Ÿæˆ HTTPS è¯ä¹¦ï¼ˆåŒåŸæ–‡æ¡£ï¼Œä½†è·¯å¾„éœ€ä¿®æ­£ï¼‰
Bash
ç¼–è¾‘
sudo mkdir -p /data/cert
cd /data/cert
sudo openssl genrsa -des3 -out server.key 2048
sudo openssl req -new -key server.key -out server.csr
sudo cp server.key server.key.org
sudo openssl rsa -in server.key.org -out server.key
sudo openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt
sudo chmod 644 server.*
5. å®‰è£… Harbor
Bash
ç¼–è¾‘
cd /usr/local/harbor
sudo ./install.sh
ä¹ã€K8s éƒ¨ç½²åº”ç”¨å¹¶æµ‹è¯•
1. éƒ¨ç½² nginx åº”ç”¨ï¼ˆä½¿ç”¨ Harbor é•œåƒï¼‰
Bash
ç¼–è¾‘
# åˆ›å»º Deployment
kubectl run nginx-deployment --image=hub.yyq.com/library/mynginx:v1 --port=80 --replicas=1

# éªŒè¯
kubectl get pods -o wide
2. æš´éœ²åº”ç”¨ä¸º Service
Bash
ç¼–è¾‘
kubectl expose deployment nginx-deployment --port=30000 --target-port=80 --type=NodePort
kubectl get svc
3. å¤–éƒ¨è®¿é—®æµ‹è¯•
Bash
ç¼–è¾‘
# åœ¨å®¿ä¸»æœºæµè§ˆå™¨è®¿é—®
http://192.168.66.20:31679  # æ›¿æ¢ä¸ºå®é™… NodePort
âœ… æˆåŠŸæ ‡å¿—ï¼šæµè§ˆå™¨æ˜¾ç¤º "Welcome to nginx!"

ğŸ”¥ å…³é”®éªŒè¯å‘½ä»¤ï¼ˆéƒ¨ç½²åå¿…åšï¼‰
Bash
ç¼–è¾‘
# 1. éªŒè¯ K8s ç‰ˆæœ¬
kubectl version --short

# 2. éªŒè¯ CRI è¿è¡Œæ—¶
sudo crictl info | grep -A 2 runtime

# 3. éªŒè¯ Flannel ç½‘ç»œ
kubectl get pods -n kube-system | grep flannel

# 4. éªŒè¯ Harbor é•œåƒä»“åº“
curl -k https://hub.yyq.com/v2/  # -k å¿½ç•¥ SSL è¯ä¹¦é”™è¯¯
```





2. åŸºäºäºŒè¿›åˆ¶æ–‡ä»¶éƒ¨ç½²

**æ­¥éª¤ï¼š**

1. æ‰‹åŠ¨ä¸‹è½½ Kubernetes ç»„ä»¶ï¼ˆå¦‚ kube-apiserverã€kube-controller-manager ç­‰ï¼‰ã€‚
2. é…ç½®æ¯ä¸ªç»„ä»¶çš„å‚æ•°å’Œå¯åŠ¨å‘½ä»¤ã€‚
3. éƒ¨ç½² etcd é›†ç¾¤ä½œä¸ºæ•°æ®å­˜å‚¨ã€‚
4. å¯åŠ¨ Kubernetes ç»„ä»¶å¹¶é…ç½®ç½‘ç»œæ’ä»¶ã€‚

**é€‚ç”¨åœºæ™¯ï¼š** é€‚åˆéœ€è¦é«˜åº¦è‡ªå®šä¹‰å’Œæ·±å…¥äº†è§£ Kubernetes å·¥ä½œåŸç†çš„ç”¨æˆ·ã€‚





[K8Sâ€”â€”å¹³å°è§„åˆ’å’Œéƒ¨ç½²æ–¹å¼ï¼ˆå°šç¡…è°·ï¼ŒäºŒè¿›åˆ¶å®‰è£…æ–¹å¼ä¸å¤ªå‹å¥½ï¼‰_å°šç¡…è°·kuberneteséƒ¨ç½²æ–‡æ¡£-CSDNåšå®¢](https://blog.csdn.net/weixin_42789698/article/details/130041994)



é”™è¯¯

```
info: node \"k8s-master01\" not found"
10æœˆ 01 20:21:21 k8s-master01 kubelet[18425]: E1001 20:21:21.782698   18425 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master01.186a5d58fd5aabfb", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<ni/>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master01", UID:"k8s-master01", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master01"}, FirstTimestamp:time.Date(2025, time.October, 1, 20, 20, 41, 230683131, time.Local), LastTimestamp:time.Date(2025, time.October, 1, 20, 20, 41, 230683131, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"k8s-master01"}': 'Post "https://192.168.66.10:6443/api/v1/namespaces/default/events": dial tcp 192.168.66.10:6443: connect: connection refused'(may retry after sleeping)
10æœˆ 01 20:21:21 k8s-master01 kubelet[18425]: E1001 20:21:21.851302   18425 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.66.10:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/k8s-master01?timeout=10s\": dial tcp 192.168.66.10:6443: connect: connection refused" interval="7s"
10æœˆ 01 20:21:22 k8s-master01 kubelet[18425]: I1001 20:21:22.007152   18425 kubelet_node_status.go:70] "Attempting to register node" node="k8s-master01"
10æœˆ 01 20:21:22 k8s-master01 kubelet[18425]: E1001 20:21:22.007825   18425 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://192.168.66.10:6443/api/v1/nodes\": dial tcp 192.168.66.10:6443: connect: connection refused" node="k8s-master01"

# 

```





é…ç½®é•œåƒåŸ

```
# è®¾ç½®é˜¿é‡Œäº‘é•œåƒæºï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
export REGISTRY=registry.aliyuncs.com/google_containers
```









#### nodeå®‰è£…k8sæ–‡æ¡£

è®¾ç½®ç›¸åŒé•œåƒ

root@k8s-node01:/etc/apt# pwd
/etc/apt



```
# 1. remove old k8s repo if exists
rm -f /etc/apt/sources.list.d/kubernetes.list

# 2. add the official k8s repo
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /  # v1.28 repo
EOF

# 3. update package index
apt-get update
```

```
apt-cache madison kubelet kubeadm kubectl
```

```
apt-get install -y \
  kubelet=1.28.2-1.1 \
  kubeadm=1.28.2-1.1 \
  kubectl=1.28.2-1.1
```

```
apt-mark hold kubelet kubeadm kubectl
```

**systemctl enable kubelet --now**









è¿™é‡Œç‰ˆæœ¬å¯¹åº”1.28.2-1.1

åŒæ—¶åœ¨unbuunruä¸Šè¿è¡Œ

````
å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯**æ”¹å†™å¹¶ä¼˜åŒ–åçš„å®Œæ•´æµç¨‹è¯´æ˜**ï¼Œé€‚é…ä½ å½“å‰çš„ç¯å¢ƒï¼ˆUbuntu 18.04 + Kubernetes v1.28.2 + é˜¿é‡Œäº‘é•œåƒæºï¼‰ï¼Œå¹¶**æ˜ç¡®æŒ‡å‡ºå…³é”®é…ç½®ä¸å¸¸è§é™·é˜±**ï¼ˆå¦‚ `NotReady` é—®é¢˜ï¼‰ï¼š

---

### ğŸš€ åˆå§‹åŒ– Master èŠ‚ç‚¹ï¼ˆä½¿ç”¨é˜¿é‡Œäº‘é•œåƒæºï¼Œé¿å…æ‹‰å–è¶…æ—¶ï¼‰

```bash
kubeadm init \
  --apiserver-advertise-address=192.168.66.10 \
  --image-repository=registry.aliyuncs.com/google_containers \
  --kubernetes-version=v1.28.2 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16
```

> âœ… è¯´æ˜ï¼š
> - `--image-repository` æŒ‡å®šé˜¿é‡Œäº‘é•œåƒä»“åº“ï¼Œé¿å…è®¿é—® `k8s.gcr.io` è¶…æ—¶
> - `--pod-network-cidr=10.244.0.0/16` ä¸º **Flannel ç½‘ç»œæ’ä»¶**çš„æ ‡å‡† CIDRï¼ˆè‹¥ç”¨ Calico ç­‰éœ€è°ƒæ•´ï¼‰
> - `--apiserver-advertise-address` å¿…é¡»æ˜¯ä½  master çš„ **å®é™…å†…ç½‘ IP**

æ‰§è¡ŒæˆåŠŸåï¼Œä¼šè¾“å‡ºç±»ä¼¼ä»¥ä¸‹çš„ `kubeadm join` å‘½ä»¤ï¼ˆ**åŠ¡å¿…ä¿å­˜ï¼**ï¼‰ã€‚

---

### ğŸ§¾ é…ç½® kubectlï¼ˆä»…åœ¨ master æ‰§è¡Œï¼‰

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ `kubectl` ç®¡ç†é›†ç¾¤ã€‚

---

### ğŸ” æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€

```bash
kubectl get nodes
```

ä½ ä¼šçœ‹åˆ° master èŠ‚ç‚¹çŠ¶æ€ä¸º **`NotReady`** â€”â€” è¿™æ˜¯**æ­£å¸¸ç°è±¡**ï¼Œå› ä¸º **CNI ç½‘ç»œæ’ä»¶å°šæœªå®‰è£…**ã€‚

> âš ï¸ æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬ masterï¼‰åœ¨æœªå®‰è£… CNI å‰éƒ½ä¼šæ˜¯ `NotReady`ï¼

---


### â• å°† Worker èŠ‚ç‚¹åŠ å…¥é›†ç¾¤

åœ¨ **æ¯ä¸ª worker èŠ‚ç‚¹**ï¼ˆå¦‚ `k8s-node01`ï¼‰ä¸Šæ‰§è¡Œä½ åœ¨ master åˆå§‹åŒ–åè·å¾—çš„ `join` å‘½ä»¤ï¼Œä¾‹å¦‚ï¼š

```bash
kubeadm join 192.168.66.10:6443 \
  --token 2g250x.30bomobd2v6s3hjm \
  --discovery-token-ca-cert-hash sha256:02a19437bd9725fc8067ed26dce92120a55918e60afc95d3c72a2564e1d76de8
```

> ğŸ’¡ å¦‚æœ token è¿‡æœŸï¼ˆé»˜è®¤ 24 å°æ—¶ï¼‰ï¼Œåœ¨ master é‡æ–°ç”Ÿæˆï¼š
> ```bash
> kubeadm token create --print-join-command
> ```

---

### âœ… æœ€ç»ˆéªŒè¯

åœ¨ master æ‰§è¡Œï¼š

```bash
kubectl get nodes
```

è¾“å‡ºåº”ç±»ä¼¼ï¼š

```
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   5m    v1.28.2
k8s-node01     Ready    <non/>          1m    v1.28.2
```

æ‰€æœ‰èŠ‚ç‚¹ `STATUS` ä¸º **`Ready`**ï¼Œè¡¨ç¤ºé›†ç¾¤å·²æ­£å¸¸è¿è¡Œï¼

---

### ğŸ”’ è¡¥å……å»ºè®®

- **ä¸è¦å¿½ç•¥ CNI å®‰è£…**ï¼šè¿™æ˜¯ `NotReady` çš„æœ€å¸¸è§åŸå› 
- **ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹æ—¶é—´åŒæ­¥**ï¼šä½¿ç”¨ `chrony` æˆ– `ntp`
- **å…³é—­ swap**ï¼ˆå·²åœ¨ kubeadm preflight æ£€æŸ¥ä¸­å¼ºåˆ¶è¦æ±‚ï¼‰ï¼š
  ```bash
  swapoff -a
  sed -i '/ swap / s/^/#/' /etc/fstab
  ```

---

ç°åœ¨ä½ çš„é›†ç¾¤å·²ç»å®Œæ•´æ­å»ºæˆåŠŸï¼ğŸ‰  
ä¸‹ä¸€æ­¥å¯ä»¥éƒ¨ç½²åº”ç”¨ã€å®‰è£… Ingressã€Metrics Server ç­‰ã€‚
````





>/> **`[kubelet-check] Initial timeout of 40s passed.`**
>/>  å¹¶ä¸”å¡åœ¨ `[wait-control-plane] Waiting for the kubelet to boot up the control plane...`
>
> è¿™è¯´æ˜ **kubelet å¯åŠ¨äº†ï¼Œä½† control plane Podsï¼ˆapiserverã€etcd ç­‰ï¼‰è¿Ÿè¿Ÿæ²¡æœ‰ Running**ã€‚





```
root@k8s-master01:/home/master# crictl pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.2
Image is up to date for sha256:cdcab12b2dd16cce4efc5dd43c082469364f19ad978e922d110b74a42eff7cce


Logs begin at Wed 2025-10-01 16:00:37 CST, end at Thu 2025-10-02 11:53:18 CST. --
10æœˆ 02 11:52:06 k8s-master01 kubelet[4104]: E1002 11:52:06.228980    4104 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master01.186a8ff2eec565b2", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<ni/>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master01", UID:"k8s-master01", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-master01 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master01"}, FirstTimestamp:time.Date(2025, time.October, 2, 11, 47, 57, 992371634, time.Local), LastTimestamp:time.Date(2025, time.October, 2, 11, 47, 57, 992371634, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"k8s-master01"}': 'Post "https://192.168.66.10:6443/api/v1/namespaces/default/events": dial tcp 192.168.66.10:6443: connect: connection refused'(may retry after sleeping)


requesting a signed certificate from the control plane: cannot create certificate signing request: Post "https://192.168.66.10:6443/apis/certificates.k8s.io/v1/certificatesigningrequests": dial tcp 192.168.66.10:6443: connect: connection refused
10æœˆ 02 11:53:18 k8s-master01 kubelet[4104]: E1002 11:53:18.618084    4104 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.66.10:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/k8s-master01?timeout=10s\": dial tcp 192.168.66.10:6443: connect: connection refused" interval="7s"
10æœˆ 02 11:53:18 k8s-master01 kubelet[4104]: I1002 11:53:18.764238    4104 kubelet_node_status.go:70] "Attempting to register node" node="k8s-master01"
10æœˆ 02 11:53:18 k8s-master01 kubelet[4104]: E1002 11:53:18.764446    4104 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://192.168.66.10:6443/api/v1/nodes\": dial tcp 192.168.66.10:6443: connect: connection refused" node="k8s-master01"


init] Using Kubernetes version: v1.28.2
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

```





> 1. **è¿æ¥è¢«æ‹’ç»**ï¼šå¤šæ¬¡å‡ºç°`dial tcp 192.168.66.10:6443: connect: connection refused`é”™è¯¯ã€‚è¿™æ„å‘³ç€Kubernetes APIæœåŠ¡å™¨ï¼ˆkube-apiserverï¼‰å¯èƒ½æ²¡æœ‰æ­£ç¡®è¿è¡Œæˆ–ç›‘å¬åœ¨æŒ‡å®šçš„IPå’Œç«¯å£ä¸Šã€‚
> 2. **æ–‡ä»¶å·²å­˜åœ¨é”™è¯¯**ï¼šå°è¯•åˆå§‹åŒ–é›†ç¾¤æ—¶ï¼ŒkubeadmæŠ¥å‘Šè¯´æŸäº›å…³é”®çš„manifestæ–‡ä»¶å·²ç»å­˜åœ¨äº`/etc/kubernetes/manifests/`ç›®å½•ä¸‹ã€‚è¿™é€šå¸¸æ„å‘³ç€ä¹‹å‰çš„å°è¯•å¯èƒ½å·²ç»åœ¨ç³»ç»Ÿä¸­ç•™ä¸‹äº†éƒ¨åˆ†é…ç½®ï¼Œå¯¼è‡´å†²çªã€‚
> 3. **ç«¯å£å ç”¨**ï¼šé”™è¯¯æŒ‡å‡º`Port 10250 is in use`ï¼Œè¿™æ˜¯kubeletä½¿ç”¨çš„å¥åº·æ£€æŸ¥ç«¯å£ã€‚å¦‚æœè¯¥ç«¯å£å·²ç»è¢«å ç”¨ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥æ˜¯ä»€ä¹ˆè¿›ç¨‹å ç”¨äº†è¿™ä¸ªç«¯å£ã€‚



```
å…³äº API Server çš„çŠ¶æ€ å½“ä½ æ£€æŸ¥ kube-apiserver çš„çŠ¶æ€æ—¶ï¼Œå¾—åˆ°äº†â€œUnit kube-apiserver.service could not be foundâ€çš„æ¶ˆæ¯ã€‚è¿™æ˜¯å› ä¸º kube-apiserver æ˜¯ä½œä¸ºä¸€ä¸ªé™æ€ Pod è¿è¡Œåœ¨ kubelet ä¸Šçš„ï¼Œè€Œä¸æ˜¯ä½œä¸ºç³»ç»ŸæœåŠ¡ç›´æ¥ç®¡ç†çš„ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æ‰€æœ‰é™æ€ Pods çš„çŠ¶æ€ï¼š
Bash
ç¼–è¾‘
crictl pods
æŸ¥æ‰¾åŒ…å« kube-apiserver åç§°çš„ Pod æ¥ç¡®è®¤å…¶çŠ¶æ€ã€‚
æ¸…ç†æ—§çš„ Kubeconfig æ–‡ä»¶ å¦‚æç¤ºæ‰€è¿°ï¼Œkubeadm reset ä¸ä¼šè‡ªåŠ¨åˆ é™¤ $HOME/.kube/config æ–‡ä»¶ã€‚å¦‚æœä½ ä¹‹å‰è¿è¡Œè¿‡ kubeadm init æˆ–è€…æœ‰å…¶ä»–é…ç½®ï¼Œåº”è¯¥æ‰‹åŠ¨åˆ é™¤æˆ–å¤‡ä»½è¿™ä¸ªæ–‡ä»¶ï¼Œç„¶åé‡æ–°ç”Ÿæˆæ–°çš„ kubeconfig æ–‡ä»¶ï¼š
Bash
ç¼–è¾‘
rm $HOME/.kube/config
mkdir -p $HOME/.kube
kubectl --kubeconfig=/etc/kubernetes/admin.conf config use-context kubernetes-admin@kubernetes
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

```
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
root@k8s-master01:/home/master# crictl pods
POD ID              CREATED             STATE               NAME                NAMESPACE           ATTEMPT             RUNTIME
root@k8s-master01:/home/master# rm $HOME/.kube/config
rm: cannot remove '/root/.kube/config': No such file or directory
root@k8s-master01:/home/master# mkdir -p $HOME/.kube
root@k8s-master01:/home/master# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
cp: cannot stat '/etc/kubernetes/admin.conf': No such file or directory

```





 è¾“å‡ºï¼š/tmp/k8s_diagnostics_<timestam/>.tar.gz ï¼ˆåŒ…å«æ‰€æœ‰æ”¶é›†æ–‡ä»¶ï¼‰



[å†å°½è‰°è¾›çš„é—®é¢˜ï¼šWaiting for the kubelet to boot up the control plane......This can take up to 4m0s-CSDNåšå®¢](https://blog.csdn.net/ygd11/article/details/129277208)



```
root@k8s-master01:/home/master# kubectl get node
E1002 13:05:29.495309    2432 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1002 13:05:29.495561    2432 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1002 13:05:29.499705    2432 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E1002 13:05:29.499971    2432 me
```



```
[sudo] password for master: 
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
eW1002 13:10:00.561454    2704 reset.go:120] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: Get "https://192.168.66.10:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s": dial tcp 192.168.66.10:6443: connect: connection refused
W1002 13:10:00.562294    2704 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: ^H^H^H^H
```



```
10æœˆ 02 13:27:38 k8s-master01 containerd[4468]: time="2025-10-02T13:27:38.708382930+08:00" level=info msg="Start cni network conf syncer for default"
10æœˆ 02 13:27:38 k8s-master01 containerd[4468]: time="2025-10-02T13:27:38.708386238+08:00" level=info msg="Start streaming server"
root@k8s-master01:/etc/containerd# 
root@k8s-master01:/etc/containerd# sudo crictl info | grep -A 5 -B 5 "registry\|systemdCgroup"
      "confDir": "/etc/cni/net.d",
      "maxConfNum": 1,
      "confTemplate": "",
      "ipPref": ""
    },
    "registry": {
      "configPath": "",
      "mirrors": {},
      "configs": {},
      "auths": {},
      "headers": {
--
    "streamServerAddress": "127.0.0.1",
    "streamServerPort": "0",
    "streamIdleTimeout": "4h0m0s",
    "enableSelinux": false,
    "selinuxCategoryRange": 1024,
    "sandboxImage": "registry.k8s.io/pause:3.6",
    "statsCollectPeriod": 10,
    "systemdCgroup": false,
    "enableTLSStreaming": false,
    "x509KeyPairStreaming": {
      "tlsCertFile": "",
      "tlsKeyFile": ""
    },
root@k8s
```





# ï¼ï¼ï¼ï¼ï¼æˆåŠŸå®‰è£…å¯¹åº”k8s

ä½¿ç”¨å®šä¹‰å¯¹åº”config

root@k8s-master01:/home/init# kubeadm version

```
kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.2", GitCommit:"89a4ea3e1e4ddd7f7572286090359983e0387b2f", GitTreeState:"clean", BuildDate:"2023-09-13T09:34:32Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}
```

kubeadm version

åŒæ—¶é…ç½®

ç»“æ„ ä¸‹è½½å¯¹åº”æœ¬åœ°é•œåƒ  --é’ˆå¯¹ç½‘ç»œå»¶è¿Ÿå¯¼è‡´æœåŠ¡å¤±æ•ˆ



ä½¿ç”¨é˜¿é‡Œäº‘ 



å–æ¶ˆswap

æ¡¥æ¥æµé‡

è®¾ç½®å®¹å™¨ CRIctlæ§åˆ¶å™¨ kubeletæœåŠ¡è¿è¡Œ

```
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.66.10 //ä¸»èŠ‚ç‚¹ip
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: k8s-master01 //ä¸»æœºå hostname æŸ¥çœ‹
  taints:
   - effect: NoSchedule
     key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.28.2 //æ ¸å¿ƒ é’ˆå¯¹ä¸»èŠ‚ç‚¹é•œåƒçš„ç‰ˆæœ¬
//é˜²æ­¢è¿œç¨‹æ‹‰å–å¤±è´¥

//CIDRæ ¼å¼
//CIDRï¼ˆClassless Inter-Domain Routingï¼‰è¡¨ç¤ºæ³•ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
networking:
  dnsDomain: cluster.local
  podSubnet: 172.7.0.0/16 //ç½‘ç»œæ’ä»¶
  serviceSubnet: 10.96.0.0/12

scheduler: {}
~                                                                                                                                                                                                                 
```



[kudeadm éƒ¨ç½² k8s_kubedam-CSDNåšå®¢](https://blog.csdn.net/Jerry00713/article/details/126440061?csdn_share_tail={"type"%3A"blog"%2C"rType"%3A"article"%2C"rId"%3A"126440061"%2C"source"%3A"Jerry00713"})







ä»èŠ‚ç‚¹ åŠ è½½å¯¹åº”æ¨¡å—

```
ternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.66.10:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:344d6fd9bec10f5c88663d7ffb4c3538cfe8efd184a580cee2a78224b47cef0c 
root@k8s-master01:/home/init# kubetctl get node

Command 'kubetctl' not found, did you mean:

  command 'kubectl' from snap kubectl (1.34.1)

See 'snap info <snapnam/>' for additional versions.

root@k8s-master01:/home/init# kubectl get node

```



```

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.


```



åŠ è½½æ¨¡å—è„šæœ¬

```sh
#!/bin/bash

set -euo pipefail

echo "[INFO] åŠ è½½ br_netfilter å†…æ ¸æ¨¡å—..."
modprobe br_netfilter

echo "[INFO] æŒä¹…åŒ–åŠ è½½ br_netfilter æ¨¡å—ï¼ˆé¿å…é‡å¯åå¤±æ•ˆï¼‰..."
cat/> /etc/modules-load.d/k8s.conf <<EOF
br_netfilter
EOF

echo "[INFO] é…ç½® sysctl å‚æ•°..."
cat/> /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

echo "[INFO] åº”ç”¨ sysctl é…ç½®..."
sysctl --system

echo "[INFO] éªŒè¯é…ç½®..."
if [[ $(sysctl -n net.bridge.bridge-nf-call-iptables) == "1" ]] && \
   [[ $(sysctl -n net.bridge.bridge-nf-call-ip6tables) == "1" ]] && \
   [[ $(sysctl -n net.ipv4.ip_forward) == "1" ]]; then
    echo "[SUCCESS] Kubernetes ç½‘ç»œå‰ç½®æ¡ä»¶å·²æ»¡è¶³ï¼"
else
    echo "[ERROR] é…ç½®æœªç”Ÿæ•ˆï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ã€‚"
    exit 1
fi
```





## CNI ç»“åˆéƒ¨ç½²çš„å¯¹åº”ip

```
# æŸ¥çœ‹å½“å‰é•œåƒ
grep image kube-flannel.yml

# æ›¿æ¢ä¸ºé˜¿é‡Œäº‘é•œåƒï¼ˆä»¥ v0.25.1 ä¸ºä¾‹ï¼‰
sed -i 's|docker.io/flannel/flannel:.*|registry.aliyuncs.com/google_containers/flannel:v0.25.1|g' kube-flannel.yml
```

[]()

> networking:
>   dnsDomain: cluster.local
>   podSubnet: 172.7.0.0/16 //ç½‘ç»œæ’ä»¶
>   serviceSubnet: 10.96.0.0/12



```
10æœˆ 02 15:26:32 k8s-master01 kubelet[10841]: E1002 15:26:32.114818   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady mes
10æœˆ 02 15:26:37 k8s-master01 kubelet[10841]: E1002 15:26:37.116014   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady mes
10æœˆ 02 15:26:42 k8s-master01 kubelet[10841]: E1002 15:26:42.118353   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady mes
10æœˆ 02 15:26:47 k8s-master01 kubelet[10841]: E1002 15:26:47.119720   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady mes
10æœˆ 02 15:26:52 k8s-master01 kubelet[10841]: E1002 15:26:52.121843   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady mes
10æœˆ 02 15:26:57 k8s-master01 kubelet[10841]: E1002 15:26:57.123354   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady mes
10æœˆ 02 15:27:02 k8s-master01 kubelet[10841]: E1002 15:27:02.125403   10841 kubelet.go:2855] "Container runtime network not ready" networkReady="Netw
```



          value: "false"
        image: ghcr.io/flannel-io/flannel:v0.27.3



```
kube-flannel.yml ä¸­å®šä¹‰äº†ï¼š
ä¸€ä¸ª DaemonSetï¼ˆkind: DaemonSetï¼‰
ä¸€ä¸ª ConfigMapï¼ˆåŒ…å« CNI é…ç½®ï¼‰
ä¸€ä¸ª ServiceAccount å’Œ RBAC æƒé™
```



```
#!/bin/bash

set -euo pipefail

FLANNEL_YAML="kube-flannel.yml"
POD_CIDR="172.7.0.0/16"
ALIYUN_REGISTRY="registry.aliyuncs.com/google_containers"


echo "[INFO] ä¿®æ”¹ Pod CIDR ä¸º $POD_CIDR..."
sed -i "s|10\.244\.0\.0/16|$POD_CIDR|g" "$FLANNEL_YAML"

echo "[INFO] æ›¿æ¢é•œåƒä¸ºé˜¿é‡Œäº‘é•œåƒ..."

# æ›¿æ¢ flannel ä¸»é•œåƒï¼ˆghcr.io/flannel-io/flannel â†’ é˜¿é‡Œäº‘ï¼‰
sed -i "s|ghcr.io/flannel-io/flannel:$.*$|$ALIYUN_REGISTRY/flannel:\1|g" "$FLANNEL_YAML"

# æ›¿æ¢ flannel-cni-plugin é•œåƒï¼ˆè¿™ä¸ªé˜¿é‡Œäº‘å¯èƒ½æ²¡æœ‰ï¼Œä½†å¯å°è¯•ç”¨ dockerhub é•œåƒæˆ–ä¿ç•™ï¼‰
# æ³¨æ„ï¼šæˆªè‡³ 2025 å¹´ï¼Œé˜¿é‡Œäº‘æš‚æœªåŒæ­¥ flannel-cni-pluginï¼Œä½†è¯¥æ’ä»¶ä½“ç§¯å°ï¼Œé€šå¸¸å¯æ‹‰å–
# å¦‚æœæ‹‰å–å¤±è´¥ï¼Œå¯æ‰‹åŠ¨åœ¨å„èŠ‚ç‚¹æ‹‰å–æˆ–ä½¿ç”¨ä»£ç†

echo "[INFO] å½“å‰ä½¿ç”¨çš„é•œåƒï¼š"
grep "image:" "$FLANNEL_YAML" | sort -u

echo "[INFO] éƒ¨ç½² Flannel åˆ°é›†ç¾¤..."
kubectl apply -f "$FLANNEL_YAML"

echo "[INFO] ç­‰å¾… Flannel Pod å¯åŠ¨ï¼ˆçº¦ 30 ç§’ï¼‰..."
sleep 10

echo "[INFO] å½“å‰ Flannel Pod çŠ¶æ€ï¼š"
kubectl get pods -n kube-flannel

echo "[INFO] èŠ‚ç‚¹çŠ¶æ€ï¼š"
kubectl get nodes

echo "[SUCCESS] Flannel éƒ¨ç½²å®Œæˆï¼è¯·è§‚å¯Ÿ 1 åˆ†é’Ÿï¼ŒèŠ‚ç‚¹åº”å˜ä¸º Readyã€‚"
```



```
ç»“åˆä¸Šä¸‹æ–‡ æ¯æ¬¡è¾“å…¥çš„k8sç‰ˆæœ¬ä¸º1-28.02kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.2", GitCommit:"89a4ea3e1e4ddd7f7572286090359983e0387b2f", GitTreeState:"clean", BuildDate:"2023-09-13T09:34:32Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}ï¼Œä½¿ç”¨containå®¹å™¨ï¼Œæœ¬åœ°å·²å®‰è£…å¥½yamlæ–‡ä»¶ï¼Œç»§ç»­å®Œå–„
```





CNIæ¯ä¸ªé•œåƒéƒ½æŒºæœ‰





scpä¼ è¾“

```
# å¯¼å…¥ flannel ä¸»é•œåƒ
ctr -n k8s.io images import ./flannel.tar

# å¯¼å…¥ cni-plugin é•œåƒ
ctr -n k8s.io images import ./flannel-cni-plugin.tar
```



æ¯ä¸ªkubeletéƒ½éœ€è¦è¿›è¡Œéƒ¨ç½²æ–‡ä»¶ é…ç½®servie

> æ˜¯çš„ï¼Œ**å®Œå…¨å¯ä»¥ä¿è¯ä¸åŒ…å«ä¸»èŠ‚ç‚¹ï¼ˆmaster/control-planeï¼‰ç‰¹æœ‰å†…å®¹** â€”â€” åªéœ€**æ’é™¤ä»… master èŠ‚ç‚¹æ‰éœ€è¦çš„å‚æ•°**ã€‚
>
> ---
>
> ### âœ… ä¸»èŠ‚ç‚¹ vs ä»èŠ‚ç‚¹ï¼šå…³é”®åŒºåˆ«
>
> | å‚æ•°                                                         | ä¸»èŠ‚ç‚¹ï¼ˆMasterï¼‰                        | ä»èŠ‚ç‚¹ï¼ˆWorkerï¼‰                   | æ˜¯å¦åº”åŒ…å«åœ¨ worker çš„ service æ–‡ä»¶ä¸­ï¼Ÿ |
> | ------------------------------------------------------------ | --------------------------------------- | ---------------------------------- | --------------------------------------- |
> | `--pod-manifest-path=/etc/kubernetes/manifests`              | âœ… ç”¨äºè¿è¡Œé™æ€ Podï¼ˆå¦‚ kube-apiserverï¼‰ | âŒ worker ä¸è¿è¡Œ control-plane ç»„ä»¶ | **âŒ ä¸åº”åŒ…å«**                          |
> | `--register-with-taints=node-role.kubernetes.io/master:NoSchedule` | âœ… ä¸»èŠ‚ç‚¹ä¸“ç”¨æ±¡ç‚¹                        | âŒ worker ä¸éœ€è¦                    | **âŒ ä¸åº”åŒ…å«**                          |
> | `--node-labels=node-role.kubernetes.io/master`               | âœ… ä¸»èŠ‚ç‚¹æ ‡ç­¾                            | âŒ                                  | **âŒ ä¸åº”åŒ…å«**                          |
> | `--bootstrap-kubeconfig`                                     | âœ… é¦–æ¬¡åŠ å…¥æ—¶éœ€è¦ï¼ˆmaster/worker éƒ½æœ‰ï¼‰  | âœ… worker åŠ å…¥é›†ç¾¤æ—¶ä¹Ÿéœ€è¦          | **âœ… åº”ä¿ç•™**                            |
> | `--kubeconfig`                                               | âœ…                                       | âœ…                                  | **âœ… ä¿ç•™**                              |
> | `--config`                                                   | âœ…                                       | âœ…                                  | **âœ… ä¿ç•™**                              |
> | `--container-runtime-endpoint`                               | âœ…                                       | âœ…                                  | **âœ… ä¿ç•™**                              |
> | `--pod-infra-container-image`                                | å¯é€‰                                    | å¯é€‰ï¼ˆä½†ä½ æ­£è¦åŠ ï¼‰                 | **âœ… æŒ‰éœ€æ·»åŠ **                          |
>
>/> ğŸ“Œ **ç»“è®º**ï¼šåªè¦**ä¸å†™ `--pod-manifest-path` å’Œ master ä¸“å±æ ‡ç­¾/æ±¡ç‚¹**ï¼Œå°±æ˜¯å¹²å‡€çš„ worker é…ç½®ã€‚
>
> ---
>
> ### âœ… çº¯å‡€çš„ä»èŠ‚ç‚¹ `kubelet.service`ï¼ˆæ— ä»»ä½• master å†…å®¹ï¼‰
>
> ```ini
> # /etc/systemd/system/kubelet.service
> [Unit]
> Description=kubelet: The Kubernetes Node Agent
> Documentation=https://kubernetes.io/docs/
> Wants=network-online.target
> After=network-online.target
> 
> [Service]
> ExecStart=/usr/bin/kubelet \
>   --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \
>   --kubeconfig=/etc/kubernetes/kubelet.conf \
>   --config=/var/lib/kubelet/config.yaml \
>   --container-runtime-endpoint=unix:///run/containerd/containerd.sock \
>   --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
> 
> Restart=always
> RestartSec=10
> StartLimitInterval=0
> KillMode=process
> 
> [Install]
> WantedBy=multi-user.target
> ```
>
> âœ… **è¿™ä¸ªé…ç½®ï¼š**
> - **æ²¡æœ‰** `--pod-manifest-path`ï¼ˆworker ä¸éœ€è¦é™æ€ Podï¼‰
> - **æ²¡æœ‰** ä»»ä½• master æ ‡ç­¾ã€æ±¡ç‚¹ã€control-plane å‚æ•°
> - **ä¿ç•™äº†** worker å¿…éœ€çš„è®¤è¯å’Œé…ç½®è·¯å¾„
> - **æ·»åŠ äº†**ä½ éœ€è¦çš„ pause é•œåƒæ›¿æ¢
>
> ---
>
> ### ğŸ” éªŒè¯æ˜¯å¦â€œå¹²å‡€â€
>
> ä½ å¯ä»¥åœ¨ä¸€ä¸ªå·²åŠ å…¥é›†ç¾¤çš„ worker èŠ‚ç‚¹ä¸Šæ‰§è¡Œï¼š
> ```bash
> # æŸ¥çœ‹å½“å‰ kubelet å®é™…å¯åŠ¨å‚æ•°ï¼ˆç”± kubeadm é»˜è®¤ç”Ÿæˆï¼‰
> ps aux | grep kubelet
> ```
>
> ä½ ä¼šå‘ç°é»˜è®¤çš„ worker kubelet **æœ¬æ¥å°±æ²¡æœ‰ `--pod-manifest-path`**ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„é…ç½®æ˜¯å‡†ç¡®çš„ã€‚
>
> ---
>
> ### ğŸ› ï¸ æœ€ç»ˆå»ºè®®
>
> - **ä½¿ç”¨ä¸Šè¿°çº¯å‡€ç‰ˆ service æ–‡ä»¶è¦†ç›–ä»èŠ‚ç‚¹çš„ `/etc/systemd/system/kubelet.service`**
> - æ‰§è¡Œï¼š
>   ```bash
>   sudo systemctl daemon-reload
>   sudo systemctl restart kubelet
>   ```
> - ä¹‹åå¯é€šè¿‡ `kubectl get node <worker-nam/> -o wide` ç¡®è®¤èŠ‚ç‚¹ä»å¤„äº `Ready` çŠ¶æ€
>
>/> âœ… è¿™æ ·æ—¢è§£å†³äº† pause é•œåƒæ‹‰å–é—®é¢˜ï¼Œåˆ**å®Œå…¨é¿å…äº†ä¸»èŠ‚ç‚¹ä¸“å±é…ç½®æ±¡æŸ“**ï¼Œå®‰å…¨ä¸”ç¬¦åˆæœ€ä½³å®è·µã€‚



[Kubernetes Pod Network CIDR_pod-network-cidr-CSDNåšå®¢](https://blog.csdn.net/jiuyanjin5740/article/details/146369614)





https://www.bookstack.cn/read/kubernetes-handbook-201910/concepts-persistent-volume.md



```

# =========================
# 2. åˆ›å»ºé™æ€ PV
# =========================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-test-volume
spec:
  capacity:
    storage: 1Gi              # PV å®¹é‡
  accessModes:
    - ReadWriteOnce           # å•èŠ‚ç‚¹å¯è¯»å†™
  persistentVolumeReclaimPolicy: Retain  # æµ‹è¯•åå¯ä¿ç•™æ•°æ®
  storageClassName: manual               # ä½¿ç”¨æ‰‹åŠ¨ç±»å‹
  hostPath:                             # ä½¿ç”¨å®¿ä¸»æœºè·¯å¾„ï¼ˆæœ¬åœ°æµ‹è¯•å¸¸ç”¨ï¼‰
    path: /mnt/data/pv-test-volume
    type: DirectoryOrCreate

---
# =========================
# 3. åˆ›å»º PVCï¼ˆç»‘å®š PVï¼‰
# =========================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-test-claim
  namespace: linhaixin
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: manual               # å¿…é¡»ä¸ PV çš„ storageClassName å¯¹åº”
  resources:
    requests:
      storage: 500Mi                     # è¯·æ±‚å°äºç­‰äº PV å®¹é‡å³å¯
  volumeName: pv-test-volume             # æŒ‡å®šç»‘å®šå“ªä¸ª PV

---
# =========================
# 4. åˆ›å»º Pod æŒ‚è½½ PVC å¹¶å†™å…¥æ•°æ®æµ‹è¯•
# =========================
apiVersion: v1
kind: Pod
metadata:
  name: pv-pvc-test-pod
  namespace: pv-test
spec:
  containers:
  - name: linhaixin.registry/linhaixin/busybox:v1.0
    image: 
    command: ["/bin/sh", "-c"]
    args: ["echo 'hello-pv-pvc'/> /data/test.txt && sleep 3600"]
    volumeMounts:
      - mountPath: /data
        name: test-volume
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: pvc-test-claim

```

> 

